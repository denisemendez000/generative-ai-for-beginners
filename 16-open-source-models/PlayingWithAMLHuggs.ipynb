{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN SOURCE HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: AbtGkG)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/microsoft/phi-4/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model microsoft/phi-4 is too large to be loaded automatically (29GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/microsoft/phi-4/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     20\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(api_key\u001b[38;5;241m=\u001b[39maccess_token)\n\u001b[1;32m     22\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m \t{\n\u001b[1;32m     24\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \t}\n\u001b[1;32m     27\u001b[0m ]\n\u001b[0;32m---> 29\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# The endpoint URL\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:892\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    870\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    871\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    872\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    889\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39mstream_options,\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    891\u001b[0m payload \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 892\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:306\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: AbtGkG)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/microsoft/phi-4/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model microsoft/phi-4 is too large to be loaded automatically (29GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "access_token = os.getenv('DAMPH4')\n",
    "# Log in using your access token\n",
    "login(token=access_token)\n",
    "\n",
    "# Your Hugging Face access token\n",
    "\n",
    "#print(access_token)\n",
    "\n",
    "# The model you want to use\n",
    "model_name = \"microsoft/phi-4\"\n",
    "\n",
    "client = InferenceClient(api_key=access_token)\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"microsoft/phi-4\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n",
    "\n",
    "# The endpoint URL\n",
    "url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "\n",
    "# The headers including the access token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# The input data\n",
    "data = {\n",
    "    \"inputs\": \"I love using Hugging Face models!\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "result = response.json()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#llama AND aml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: phi4\n",
      "Model type: chat-completion\n",
      "Model provider name: Phi\n",
      "Response: Estimating the total number of languages in the world is a complex task due to differing criteria for what constitutes a distinct language versus a dialect. However, the Ethnologue, a comprehensive reference work cataloging all of the world’s known living languages, lists approximately 7,151 languages as of its latest edition. It's important to note that this number can vary depending on the source and the criteria used for classification.\n",
      "\n",
      "Languages are constantly evolving, with some becoming extinct while others may emerge or be recognized as distinct. Additionally, political, cultural, and social factors can influence how languages are classified.\n",
      "Model: phi4\n",
      "Usage:\n",
      "\tPrompt tokens: 25\n",
      "\tTotal tokens: 145\n",
      "\tCompletion tokens: 120\n",
      "Model name: Llama-2-70b-chat\n",
      "Model type: chat-completion\n",
      "Model provider name: Meta\n",
      "Response: Estimating the number of languages in the world can be somewhat complex due to varying definitions of what constitutes a distinct language versus a dialect. According to Ethnologue, a comprehensive reference work cataloging all of the world's known living languages, there are approximately 7,151 languages spoken globally as of their 24th edition, published in 2023. However, other sources and linguistic studies may offer slightly different numbers, often ranging between 6,000 and 7,500 languages.\n",
      "\n",
      "It's important to note that many of these languages are spoken by very small communities and are at risk of becoming extinct. Language preservation efforts are crucial to maintaining linguistic diversity, as languages carry cultural heritage, knowledge, and identity.\n",
      "Model: phi4\n",
      "Usage:\n",
      "\tPrompt tokens: 25\n",
      "\tTotal tokens: 170\n",
      "\tCompletion tokens: 145\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Access the API key\n",
    "papi_key = os.getenv('PHI4DAM')\n",
    "pendpoint = os.getenv('PHI_ENDPOINT')\n",
    "pendeployment=os.getenv('PHI_DEPLOYMENT')\n",
    "\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=pendpoint,\n",
    "    credential=AzureKeyCredential(papi_key),\n",
    ")\n",
    "\n",
    "model_info = client.get_model_info()\n",
    "print(\"Model name:\", model_info.model_name)\n",
    "print(\"Model type:\", model_info.model_type)\n",
    "print(\"Model provider name:\", model_info.model_provider_name)\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"How many languages are in the world?\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\\tCompletion tokens:\", response.usage.completion_tokens)\n",
    "\n",
    "\n",
    "lapi_key =  os.getenv('DAMLLAMA')\n",
    "lendpoint=\"https://Llama-2-70b-chat-gpezz.eastus.models.ai.azure.com\"\n",
    "endpoint_name=os.getenv('LLAMA_DEPLOYMENT')\n",
    "\n",
    "client2 = ChatCompletionsClient(\n",
    "    endpoint=lendpoint,\n",
    "    credential=AzureKeyCredential(lapi_key),\n",
    ")\n",
    "\n",
    "model_info = client2.get_model_info()\n",
    "print(\"Model name:\", model_info.model_name)\n",
    "print(\"Model type:\", model_info.model_type)\n",
    "print(\"Model provider name:\", model_info.model_provider_name)\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"How many languages are in the world?\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKU)P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cosmos\n",
      "  Downloading azure_cosmos-4.9.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-cosmos)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from azure-cosmos) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-cosmos) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from azure-core>=1.30.0->azure-cosmos) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-cosmos) (2024.8.30)\n",
      "Downloading azure_cosmos-4.9.0-py3-none-any.whl (303 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Installing collected packages: azure-core, azure-cosmos\n",
      "Successfully installed azure-core-1.32.0 azure-cosmos-4.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Access the API key\n",
    "lapi_key =  os.getenv('DAMLLAMA')\n",
    "lendpoint = os.getenv('LLAMA_ENDPOINT')\n",
    "lendpoint=\"https://Llama-2-70b-chat-gpezz.eastus.models.ai.azure.com/score\"\n",
    "endpoint_name=os.getenv('LLAMA_DEPLOYMENT')\n",
    "#ml_client = MLClient(\n",
    "#    credential=InteractiveBrowserCredential(tenant_id=\"fd8c2226-bc4e-4d76-ab2f-ca18b85b7d6d\"),\n",
    "#    subscription_id=\"42c91282-2c0e-4289-bbf4-045ce4b2b6e5\",\n",
    "#    resource_group_name=\"AiLearning\",\n",
    "#    workspace_name=\"damaml2\",\n",
    "#)\n",
    "#\n",
    "#endpoint = ml_client.online_endpoints.get(endpoint_name).scoring_uri\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import uuid\n",
    "\n",
    "client_request_id = str(uuid.uuid4())\n",
    "\n",
    "# The headers including the API key\n",
    "lheaders = {\n",
    "    \"Authorization\": f\"Bearer {lapi_key}\",\n",
    "       \"Content-Type\": \"application/json\",\n",
    "        \"x-ms-client-request-id\": client_request_id\n",
    "}\n",
    "\n",
    "pheaders = {\n",
    "    \"Authorization\": f\"Bearer {papi_key}\",\n",
    "       \"Content-Type\": \"application/json\",\n",
    "        \"x-ms-client-request-id\": client_request_id\n",
    "}\n",
    "\n",
    "# The input data\n",
    "data = {\n",
    "    \"inputs\": \"I love using LLaMA models!\"\n",
    "}\n",
    "\n",
    "print(\"PHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\")\n",
    "response2 = requests.post(pendpoint, headers=pheaders, json=json.dumps(data))\n",
    "#print(f\"Status Code: {response.status_code}\")\n",
    "print(response2)\n",
    "\n",
    "\n",
    "# Make the request\n",
    "print(\"LLLAMA\")\n",
    "response1 = requests.post(lendpoint, headers=lheaders, json=json.dumps(data))\n",
    "\n",
    "print(response1)\n",
    "#print(f\"Status Code: {response.status_code}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
